
<p>再談更新節奏。很多團隊喜歡頻繁小更，覺得 agile；但對策略層來說，過快更新會讓歸因失真。建議採「固定節點 + 緊急通道」雙軌：一般策略每 24 或 48 小時一更，配套完整報表與對照；高風險事件走緊急通道，可立即回滾或熱修。更新包內必須包含三項：變更摘要、預期影響、退場條件。沒有退場條件的更新，等於把失敗成本外包給營運同仁。</p>
<p>很多人問：線上回饋能不能直接訓練模型？可以，但不該是第一步。先把 policy 層做好，再考慮 model-level tuning。原因很務實：policy 調整成本低、回滾快、可解釋；模型微調成本高、驗證慢、風險半徑大。若你連政策層的反饋治理都沒穩住，直接下模型微調，通常只是更快放大偏差。成熟路線應該是：先優化 routing/prompt/tool constraints，再對穩定收益模式做小規模微調驗證。</p>
<p>落地上可用 14 天啟動版。D1-D2 定義事件詞彙與風險分層；D3-D4 接入關鍵事件（採納、中斷、人工修訂、風險觸發）；D5-D6 建立翻譯層，把事件轉成評估分數；D7-D8 建 update gate 與灰度規則；D9-D10 上線 rollback 監控；D11-D12 做 counterfactual replay；D13-D14 跑第一次策略更新並做 postmortem。這一輪做完，你就從「觀察系統」升級到「可控學習系統」。</p>
<p>對外技術傳播者也該重視這題。因為很多內容都在談「如何讓 Agent 更聰明」，但很少談「如何讓 Agent 在真實互動中不被帶歪」。後者才是商用門檻。你若能把這類看不見的工程講清楚，會建立更深的專業信任：你不是只會介紹新名詞，而是能解決上線後的真問題。長期追隨最終靠的不是新奇感，而是穩定可信的判斷品質。</p>
<p>最後總結一句：Online Eval Loop 的目標不是讓系統追著使用者行為跑，而是讓系統在真實回饋中保持可解釋、可控制、可修正。能做到這點，Agent 才能在長期營運中持續提升，而不是在短期指標裡自我感覺良好。下一篇我會接著寫「Policy Update Gate 的設計細節：灰度比例、風險閾值與自動回退策略模板」。References（方向）：NIST AI RMF、SRE rollback best practices、off-policy evaluation 文獻、LLM safety monitoring 實務指南。</p>
</main></body></html>
