<!doctype html>
<html lang="zh-Hant">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Synthetic Data for Evals：加速 Agent 評估而不放大偏差</title>
  <meta name="description" content="對外深度分析：如何用合成案例提升 Agent 評估覆蓋率，同時控制偏差與誤導風險。" />
</head>
<body>
<main>
<h1>Synthetic Data for Evals：加速 Agent 評估而不放大偏差</h1>
<p>2026 年做 AI Agent 的團隊幾乎都遇到同一個瓶頸：評估資料不夠。不是因為不知道要評估什麼，而是因為真實案例蒐集慢、標註貴、更新跟不上產品節奏。於是很多團隊陷入兩難：要嘛用少量真實案例硬撐，導致評估盲區大；要嘛停在主觀測試，做不出可持續優化。這時 synthetic data（合成資料）看起來像解藥：生成快、成本低、可客製。但它同時也是雙面刃——用得好可以放大評估效率，用不好會把偏差系統化，讓你在錯誤方向上越跑越快。</p>
<p>我的核心觀點是：<strong>合成資料不是用來取代真實資料，而是用來放大評估覆蓋率</strong>。這句話看似基本，實務上卻最常被違反。許多團隊把 synthetic set 當主體，真實 set 當附錄，結果指標看起來漂亮，上線後卻大量翻車。原因在於模型生成資料天然會帶有模型偏好與語言慣性，你如果不做治理，最終評估的是「模型如何回答自己喜歡的題目」，而不是「模型如何面對真實世界的髒問題」。</p>
<p>先釐清 synthetic data 在 eval 裡真正該做的三件事。第一，補齊稀有場景覆蓋：真實資料裡少見但高風險的案例（例如多跳衝突、規格邊界、模糊指令）可以用合成方式擴充。第二，做結構化壓力測試：同一任務維度下，系統性改變難度、噪音、上下文長度，觀察性能曲線。第三，追蹤策略漂移：當你更新 prompt、routing policy、tool schema 時，用固定模板生成對照組，快速偵測退化。這三件事都屬於「加速評估工程」，不是替代真實監測。</p>
<p>如果要避免偏差放大，你需要先建立資料分層。最少分成三層：Real Core（真實核心集）、Synthetic Expansion（合成擴展集）、Adversarial Stress（對抗壓測集）。Real Core 是決策錨點，比例不一定最大，但權重必須最高；Synthetic Expansion 用來補場景密度；Adversarial Stress 專門測脆弱點。實務上我建議在週期評估中，報表必須分層展示，禁止把三層混成單一總分。混分看起來漂亮，但會掩蓋真實風險。</p>
<p>接著談生成方法。很多人直接用一個 prompt 叫模型「幫我生 1000 筆測資」，這通常是災難起點。正確做法是模板化生成：先定義任務語義框架（目標、限制、風險等級、輸出格式），再定義變異軸（語氣、噪音、歧義、資訊缺口、對抗性），最後做分桶抽樣，確保分布可控。你要追求的是可解釋覆蓋，不是數量堆疊。100 筆高品質可追溯的合成測資，通常比 5000 筆隨機文本更有用。</p>
<p>為了讓 synthetic eval 可審計，至少要保存五個欄位：generation prompt version、generator model version、constraint profile、seed/temperature、post-filter rule。這些欄位的作用很直接：當某批測資結果異常時，你能回溯「是模型變了、模板變了、還是過濾規則變了」。沒有這些 metadata，你會在評估異常時陷入黑盒猜測，浪費大量人力。</p>
<p>很多團隊忽略的一點是「語義污染」。如果你用同一個模型既產生測資又被測試，會有同分佈偏好，導致測試看似容易。解法不是完全禁用同模型，而是做交叉生成：至少兩個來源（不同模型或不同模板策略）生成，再由第三方規則與人工抽樣審核。你要降低的是同源偏差（source-coupled bias），否則指標會高估真實能力。</p>
<p>在 Agent 場景中，synthetic eval 更要關注工具鏈行為，而不只是文字正確性。你應該生成包含 tool-call 分岔的案例：API timeout、回傳欄位缺失、版本不一致、權限拒絕、部分成功等，觀察 agent 是否能安全降級。這些情境在真實資料裡往往稀疏，但對穩定性影響極大。若 synthetic set 不含這些 failure mode，你其實沒在測 Agent，只在測語言模型。</p>
<p>再講評估指標。除了任務成功率與人工修訂率，我建議加三個指標：Coverage Fidelity（合成資料是否真的覆蓋目標場景）、Bias Amplification Score（合成資料是否放大某類偏好）、Transfer Validity（合成集上的提升是否能轉移到真實集）。其中 Transfer Validity 最關鍵：如果 synthetic 上升、real core 不動甚至下降，代表你優化方向錯了。這時要停下來修資料策略，不是再堆更多合成樣本。</p>
<p>落地節奏可以用 14 天版本。D1-D2 定義分層架構與場景清單；D3-D5 建立模板與變異軸；D6-D7 生成首批資料並做人工抽樣質檢；D8-D9 上線分層報表；D10-D11 做交叉生成與對抗補集；D12-D13 跑一次完整 A/B；D14 做遷移驗證（synthetic → real）。這個流程重點不是快，而是每一步可回溯。你可以快，但不能黑箱。</p>
<p>對技術傳播者而言，這題特別值得講，因為外界常把 synthetic data 簡化成「便宜的真實資料」。這是危險說法。更精準的描述是：synthetic data 是一種評估放大器，它會放大你原有的資料治理能力。如果治理好，效率倍增；治理差，偏差倍增。專業形象的關鍵就在這裡：你不只告訴大家工具能做什麼，也告訴大家在什麼條件下會出事。</p>
<p>再補一個常被忽略的治理細節：synthetic data 也需要「資料退場機制」。很多團隊會把早期生成的測資永久保留，結果任務與產品都變了，舊測資仍在主報表中占權重，導致評估方向落後現況。建議每批合成資料都附 TTL（time-to-live）與適用範圍標記，超過期限或任務條件改變後自動降權，避免歷史包袱污染當前決策。這個做法對快速迭代團隊特別重要，因為資料新鮮度本身就是評估品質的一部分。</p>
<p>另外，若你要把 synthetic eval 用在多語系或跨文化產品，務必做語境多樣性審核。很多合成流程在繁中/英文切換時會出現隱性偏差：某些語言版本語氣較保守，某些版本偏向過度肯定，最後讓策略調參在不同語言上表現不一致。這不是翻譯問題，而是語用分佈問題。解法是為不同語言建立最小 real core 子集，並要求 synthetic 生成遵守語境約束，不可直接複製單一語系模板。否則你得到的是看似全球化、實則單語偏置的評估體系。</p>
<p>還有一個實務上很有價值的方法：把 synthetic 案例分成「教學型」與「診斷型」。教學型案例用來穩定基本能力，避免系統忘記核心規則；診斷型案例用來專打脆弱點，逼出邊界錯誤。兩者的生成策略應該不同：教學型要高一致、低噪音；診斷型要高變異、可控對抗。若全部混在一起，你很難知道退化是因為基礎能力下降還是邊界風險上升。這種分層能讓你的優化更精準，也能讓跨部門溝通更有效率。</p>
<p>最後給一個實務結論：任何 synthetic eval 改進都必須經過 real core 驗證門檻，否則一律視為「未證實提升」。這條規則簡單但有效，能大幅降低錯誤優化。2026 年真正成熟的 Agent 團隊，不是誰最會生成資料，而是誰最會治理資料與證明轉移有效性。References（方向）：NIST AI RMF、evaluation set design best practices、distribution shift 文獻、LLM red-teaming 實務指南。</p>
</main>
</body>
</html>
